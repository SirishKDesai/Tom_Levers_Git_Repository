---
title: "Stat 6021: HW Set 5"
author: "Tom Lever"
date: 09/29/22
output:
  pdf_document: default
  html_document: default
urlcolor: blue
linkcolor: red
---

1.  For this question, we will use the `cornnit` data set from the `faraway` package.
    Be sure to install and load the `faraway` package first, and then load the data set.
    The data explore the relationship between corn yields in bushels per acre and nitrogen fertilizer applications in pounds per acre
    in a study carried out in Wisconsin.
    
    ```{r, eval=TRUE, echo=TRUE, results="show", message=FALSE}
    library(faraway)
    data_set <- faraway::cornnit
    ```
    
    (a) What is the response variable and predictor for this study?
        Create a scatterplot of the data and interpret the scatterplot.
        
        The response variable and predictor variable for this study are corn yield in bushels per acre and nitrogen fertilizer applications
        in pounds per acre, respectively.
        
        ```{r, eval=TRUE, echo=TRUE, results="show", message=FALSE}
        library(ggplot2)
        ggplot(data_set, aes(x = nitrogen, y = yield)) +
            geom_point(alpha = 0.2) +
            geom_smooth(method = "lm", se = FALSE) +
            labs(
                x = "nitrogen fertilizer applications (pounds per acre)",
                y = "yields (bushels per acre)",
                title = "Yields vs. Nitrogen Fertilizer Applications"
            ) +
            theme(
                plot.title = element_text(hjust = 0.5),
                axis.text.x = element_text(angle = 0)
            )
        ```
        
        Generally, there appears to be an increasing association between nitrogen fertilizer applications and yields.
        The relationship between response / yields $y$ and predictor / regressor / nitrogen fertilizer applications $x$ appears nonlinear,
        perhaps logarithmic.
        
        Assumptions for simple linear regression appear to be not met.
        1. The assumption that the relationship between response / yields $y$ and
           predictor / regressor / nitrogen fertilizer applications $x$ is linear, at least approximately, is not met.
           The relationship appears to be nonlinear.
        2. The assumption that the error term $\epsilon$ of the linear model has mean $0$ is not met.
           Observations are not scattered evenly around the fitted line.
        3. The assumption that the error term $\epsilon$ of the linear model has constant variance is not met.
           The vertical variation of observations is not constant.
        
    (b) Fit a linear regression without any transformations. Create the corresponding residual plot.
        Based only on the residual plot, what transformation will you consider first? Be sure to explain your reason.
        
        ```{r, eval=TRUE, echo=TRUE, results="show", message=FALSE}
        library(TomLeversRPackage)
        linear_model <- lm(yield ~ nitrogen, data = data_set)
        print(summarize_linear_model(linear_model))
        
        ggplot(
            data.frame(
                externally_studentized_residuals = linear_model$residuals,
                predicted_yields = linear_model$fitted.values
            ),
            aes(x = predicted_yields, y = externally_studentized_residuals)
        ) +
            geom_point(alpha = 0.2) +
            geom_hline(yintercept = 0, color = "red") +
            labs(
                x = "predicted yields (bushels per acre)",
                y = "externally studentized residual (bushels per acre)",
                title = "Externally Studentized Residuals vs. Predicted Yields"
            ) +
        theme(
            plot.title = element_text(hjust = 0.5),
            axis.text.x = element_text(angle = 0)
        )
        ```
        
        Assumptions for simple linear regression appear to be not met.
        1. The assumption that the relationship between response / yields $y$ and
           predictor / regressor / nitrogen fertilizer applications $x$ is linear, at least approximately, is not met.
        2. The assumption that the error term $\epsilon$ of the linear model has mean $0$ is not met.
           Residuals are not evenly scattered around $e = 0$; residuals follow a curve.
        3. The assumption that the error term $\epsilon$ of the linear model has constant variance is not met.
           Residuals do not have similar vertical variation across $e = 0$.
           
        Since assumption 3, that the error term $\epsilon$ of the linear model has constant variance, is not met,
        we transform the predicted reponse / yields.
        Since we are correcting for the error term $\epsilon$'s non-constant variance, we use a power transformation $y^\lambda$,
        where $\lambda$ is a parameter determined below by the Box-Cox Method.
           
    (c) Create a Box-Cox plot for the profile of log-likelihoods. How does this plot aid in your data transformation?
   
        Assumptions for simple linear regression appear to be not met.
        1. The assumption that the relationship between response / yields $y$ and
           predictor / regressor / nitrogen fertilizer applications $x$ is linear, at least approximately, is not met.
           A Box-Cox plot suggests a maximum-likelihood estimate of parameter $\lambda$ around $3$, not $1$.
   
        ```{r, eval=TRUE, echo=TRUE, results="show", message=FALSE}
        result_of_Box_Cox_Method <- perform_Box_Cox_Method(
            linear_model,
            vector_of_values_of_lambda = seq(1, 5, 0.1)
        )
        print(result_of_Box_Cox_Method$maximum_likelihood_estimate_of_parameter_lambda)
        ```
        
        For the set of observations of yields and nitrogen fertilizer applications,
        the maximum-likelihood estimate of $\lambda$ is close to a whole, within-confidence-interval parameter $\lambda = 3$.
        Since parameter $\lambda = 3 \neq 0$, we use the transformation $\hat{y}' = \hat{y}^\lambda = \hat{y}^3$.
       
    (d) Perform the necessary transformation of the data. Refit the regression with the transformed variable and
        assess the regression assumptions. You may have to apply transformations a number of times.
        Be sure to explain the reason behind each of your transformations.
        
        ```{r, eval=TRUE, echo=TRUE, results="show", message=FALSE}
        library(dplyr)
        data_set <- data_set %>% mutate(cubed_yields = yield^3)
        ggplot(data_set, aes(x = nitrogen, y = cubed_yields)) +
            geom_point(alpha = 0.2) +
            geom_smooth(method = "lm", se = FALSE) +
            labs(
                x = "nitrogen fertilizer applications (pounds per acre)",
                y = "cubed yields",
                title = "Cubed Yields vs. Nitrogen Fertilizer Applications"
            ) +
        theme(
            plot.title = element_text(hjust = 0.5),
            axis.text.x = element_text(angle = 0)
        )
        
        linear_model_with_cubed_yields <- lm(cubed_yields ~ nitrogen, data = data_set)
        print(summarize_linear_model(linear_model_with_cubed_yields))
        
        ggplot(
            data.frame(
                externally_studentized_residuals = linear_model_with_cubed_yields$residuals,
                predicted_yields = linear_model_with_cubed_yields$fitted.values
            ),
            aes(x = predicted_yields, y = externally_studentized_residuals)
        ) +
            geom_point(alpha = 0.2) +
            geom_hline(yintercept = 0, color = "red") +
            labs(
                x = "predicted yields (bushels per acre)",
                y = "externally studentized residual (bushels per acre)",
                title = "Externally Studentized Residuals vs. Predicted Yields"
            ) +
        theme(
            plot.title = element_text(hjust = 0.5),
            axis.text.x = element_text(angle = 0)
        )
        ```
        
        Assumptions for simple linear regression appear to be not met.
        1. The assumption that the relationship between response / yields $y$ and
           predictor / regressor / nitrogen fertilizer applications $x$ is linear, at least approximately, is not met.
           The relationship appears to be nonlinear.
        2. The assumption that the error term $\epsilon$ of the linear model has mean $0$ is not met.
           Observations are not scattered evenly around the fitted line.
           Residuals may be evenly scattered aroun $e = 0$.
        3. The assumption that the error term $\epsilon$ of the linear model has constant variance is not met.
           The vertical variation of observations is not constant.
           Residuals may be evenly scattered aroun $e = 0$.

        Because a cube-root function roughly fits the above scatterplot of cubed yields vs. nitrogen fertilizer applications,
        we transform the predictor / nitrogen fertilizer application $x$.
           
        ```{r, eval=TRUE, echo=TRUE, results="show", message=FALSE}
        library(dplyr)
        data_set <- data_set %>% mutate(cube_rooted_applications = nitrogen^(1/3))
        ggplot(data_set, aes(x = cube_rooted_applications, y = cubed_yields)) +
            geom_point(alpha = 0.2) +
            geom_smooth(method = "lm", se = FALSE) +
            labs(
                x = "cube rooted nitrogen fertilizer applications",
                y = "cubed yields",
                title = paste(
                    "Cubed Yields vs.\n",
                    "Cube Rooted Nitrogen Fertilizer Applications",
                    sep = ""
                )
            ) +
        theme(
            plot.title = element_text(hjust = 0.5),
            axis.text.x = element_text(angle = 0)
        )
        
        linear_model_with_cubed_yields <- lm(cubed_yields ~ nitrogen, data = data_set)
        print(summarize_linear_model(linear_model_with_cubed_yields))
        
        ggplot(
            data.frame(
                externally_studentized_residuals = linear_model_with_cubed_yields$residuals,
                predicted_yields = linear_model_with_cubed_yields$fitted.values
            ),
            aes(x = predicted_yields, y = externally_studentized_residuals)
        ) +
            geom_point(alpha = 0.2) +
            geom_hline(yintercept = 0, color = "red") +
            labs(
                x = "predicted yields (bushels per acre)",
                y = "externally studentized residual (bushels per acre)",
                title = "Externally Studentized Residuals vs. Predicted Yields"
            ) +
        theme(
            plot.title = element_text(hjust = 0.5),
            axis.text.x = element_text(angle = 0)
        )
        
        acf(
            linear_model_with_cubed_yields$residuals,
            main = "ACF Value vs. Lag for Logarithmicized Linear Model"
        )
        
        qqnorm(linear_model_with_cubed_yields$residuals)
        qqline(linear_model_with_cubed_yields$residuals, col = "red")
        ```
        
        Some assumptions for simple linear regression appear to be met.
        1. The assumption that the relationship between response / yields $y$ and
           predictor / regressor / nitrogen fertilizer applications $x$ is linear, at least approximately, is met.
           The relationship appears to be linear.
        2. The assumption that the error term $\epsilon$ of the linear model has mean $0$ is met.
           Observations are scattered evenly around the fitted line.
           Residuals may be evenly scattered aroun $e = 0$.
        3. The assumption that the error term $\epsilon$ of the linear model has constant variance is met.
           The vertical variation of observations is constant.
           Residuals may be evenly scattered aroun $e = 0$.
        4. The assumption that the errors $\epsilon_i$ / residuals $e_i$ are uncorrelated is not met.
           The ACF value for lag $0$ is always $1$; the correlation of the vector of residuals with itself is always 1.
           Since the ACF value for lag $12$ is significant, we have sufficient evidence to reject a null hypothesis that
           the residuals of the linear model are uncorrelated.
           We have sufficient evidence to conclude that the residuals of the linear model are correlated.
           We have sufficient evidence to conclude that
           the assumption that the errors $\epsilon_i$ / residuals $e_i$ are uncorrelated is not met.
           After searching the Web, we conclude that there is no background information on the cornnit data set of the faraway package.
           Observations may have been taken non-randomly from specific fields.
           Observations may depend on soil modifications due to previously fertilizing and growing corn crops.
        5. Assumptions that the errors $\epsilon_i$ / residuals $e_i$ are normally distributed is met.
           A linear model is robust to these assumptions.
           Given that observations in a plot of sample quantiles versus theoretical quantiles for the residuals
           of the linear model lie close to a line of best fit,
           the probability vs. externally studentized residuals plot / distribution is normal.
           The assumption that the errors $\epsilon_i$ / residuals $e_i$ are normally distributed is met.
        
2. A chemist studied the concentration of a solution $y$ over time $x$ by fitting a simple linear regression.
   The scatterplot of the dataset and the residual plot from the regression model are shown in the prompt for this homework.
   
   (a) Based on these two scatterplots, would you recommend transforming the predictor $x$ or the response $y$ first?
   
       The assumption that the error term $\epsilon$ of the linear model has constant variance is not met.
       Residuals do not have similar vertical variation across $e = 0$.
       
       Since the assumption that the error term $\epsilon$ of the linear model has constant variance, is not met,
       we transform the predicted reponse / concentration first.
       Transforming the predicted response / concentration may also influence the linear model's compliance with
       an assumption that the mean of errors is $0$.
       After transforming the predicted response / concentration, if ensuring the linear model's compliance with
       an assumption that the mean of errors is $0$ requires a transformation, transform the predictor / time.
       
   (b) The profile log-likelihoods for the parameter $\lambda$ of the Box-Cox power transformation is shown in the prompt for this homework.
       Your classmate says that you should apply a log transformation to the response variable first.
       Do you agree with your classmate? Be sure to justify your answer.
   
       Yes. For the set of observations of concentrations and times, the maximum-likelihood estimate of $\lambda$
       is close to a whole, within-confidence-interval parameter $\lambda = 0$.
       Since parameter $\lambda = 0$, we use the transformation $\hat{y}' = ln\left(\hat{y}\right)$.
       
   (c) Your classmate is adamant on applying the log transformation to the response variable and fits the regression model.
       The R output is shown in the prompt for this homework.
       Write down the estimated regression equation for this model.
       How do we interpret the regression coefficients $\hat{\beta}_1$ and $\hat{\beta}_0$ in context?
       
       $$\hat{\beta}_0 = 1.50792$$
       $$\hat{\beta}_1 = -0.44993 \ \frac{1}{time \ unit}$$
       $$ln\left(\hat{y}\right) = \hat{\beta}_0 + \hat{\beta}_1 x$$
       $$\hat{y} = exp\left(\hat{\beta}_0 + \hat{\beta}_1 x\right)$$
       $$\hat{y}_+ = exp\left(\hat{\beta}_0 + \hat{\beta}_1 (x + 1)\right)$$
       $$\hat{y}_+ = exp\left(\hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_1\right)$$
       $$\hat{y}_+ = exp\left(\hat{\beta}_0 + \hat{\beta}_1 x\right) exp\left(\hat{\beta}_1\right)$$
       $$\hat{y}_+ = \hat{y} \ exp\left(\hat{\beta}_1\right)$$
       
       When predictor variable / time $x$ increases by one unit, the predicted response / concentration $\hat{y}$ increases by a factor of
       $exp\left(\hat{\beta}_1\right)$.